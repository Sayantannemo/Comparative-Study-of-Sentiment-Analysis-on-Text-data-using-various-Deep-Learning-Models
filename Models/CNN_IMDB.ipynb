{"cells":[{"cell_type":"code","execution_count":null,"id":"a0b04ac5-e7d6-41be-a843-3ac306dfb5a8","metadata":{"tags":[],"id":"a0b04ac5-e7d6-41be-a843-3ac306dfb5a8"},"outputs":[],"source":["import pandas as pd    # to load dataset\n","import numpy as np     # for mathematic equation\n","from nltk.corpus import stopwords   # to get collection of stopwords\n","from sklearn.model_selection import train_test_split       # for splitting dataset\n","from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n","from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n","from tensorflow.keras.models import Sequential     # the model\n","from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout,Conv1D,GlobalMaxPooling1D # layers of the architecture\n","from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n","from tensorflow.keras.models import load_model   # load saved model\n","import re"]},{"cell_type":"code","execution_count":null,"id":"5dc70862-07c6-42c7-ad39-b5569e88cce6","metadata":{"tags":[],"id":"5dc70862-07c6-42c7-ad39-b5569e88cce6","outputId":"cd9752df-69f9-4082-bd5f-b2ee178ed772"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                  review sentiment\n","0      One of the other reviewers has mentioned that ...  positive\n","1      A wonderful little production. <br /><br />The...  positive\n","2      I thought this was a wonderful way to spend ti...  positive\n","3      Basically there's a family where a little boy ...  negative\n","4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n","...                                                  ...       ...\n","49995  I thought this movie did a down right good job...  positive\n","49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n","49997  I am a Catholic taught in parochial elementary...  negative\n","49998  I'm going to have to disagree with the previou...  negative\n","49999  No one expects the Star Trek movies to be high...  negative\n","\n","[50000 rows x 2 columns]\n"]}],"source":["data = pd.read_csv('/content/drive/MyDrive/SetModels/IMDB Dataset.csv')\n","\n","print(data)"]},{"cell_type":"code","execution_count":null,"id":"f8d69293-cbd0-4148-a2e3-3e6c97ba2ceb","metadata":{"tags":[],"id":"f8d69293-cbd0-4148-a2e3-3e6c97ba2ceb","outputId":"18f9c87d-9d17-49e3-f2f4-b52f6a2ae90b"},"outputs":[{"name":"stdout","output_type":"stream","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["english_stops = stopwords.words('english')\n","print(english_stops)"]},{"cell_type":"code","execution_count":null,"id":"1d0158c9-2cb8-4bd3-a091-b79fd47a12a1","metadata":{"tags":[],"id":"1d0158c9-2cb8-4bd3-a091-b79fd47a12a1","outputId":"851a1622-2d73-4269-ae8b-2208aa165b06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reviews\n","0        [one, reviewers, mentioned, watching, oz, epis...\n","1        [a, wonderful, little, production, the, filmin...\n","2        [i, thought, wonderful, way, spend, time, hot,...\n","3        [basically, family, little, boy, jake, thinks,...\n","4        [petter, mattei, love, time, money, visually, ...\n","                               ...                        \n","49995    [i, thought, movie, right, good, job, it, crea...\n","49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n","49997    [i, catholic, taught, parochial, elementary, s...\n","49998    [i, going, disagree, previous, comment, side, ...\n","49999    [no, one, expects, star, trek, movies, high, a...\n","Name: review, Length: 50000, dtype: object \n","\n","Sentiment\n","0        1\n","1        1\n","2        1\n","3        0\n","4        1\n","        ..\n","49995    1\n","49996    0\n","49997    0\n","49998    0\n","49999    0\n","Name: sentiment, Length: 50000, dtype: int64\n"]}],"source":["def load_dataset():\n","    df = pd.read_csv('IMDB Dataset.csv')\n","    x_data = df['review']       # Reviews/Input\n","    y_data = df['sentiment']    # Sentiment/Output\n","\n","    # PRE-PROCESS REVIEW\n","    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n","    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n","    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words and spliting\n","    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n","\n","    # ENCODE SENTIMENT -> 0 & 1\n","    y_data = y_data.replace('positive', 1)\n","    y_data = y_data.replace('negative', 0)\n","\n","    return x_data, y_data\n","\n","x_data, y_data = load_dataset()\n","\n","print('Reviews')\n","print(x_data, '\\n')\n","print('Sentiment')\n","print(y_data)"]},{"cell_type":"code","execution_count":null,"id":"3821aec3-5a77-4730-9014-4aaabc8714e1","metadata":{"tags":[],"id":"3821aec3-5a77-4730-9014-4aaabc8714e1","outputId":"206f3390-1baa-4249-bed3-059a254d81c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Set\n","5636     [i, watched, almost, gundam, mech, anime, show...\n","25192    [i, love, bad, movies, not, often, entertainin...\n","4748     [given, title, outlandish, box, art, i, ready,...\n","7084     [this, one, seriously, disturbed, movie, even,...\n","1831     [although, like, cricket, seen, movie, years, ...\n","                               ...                        \n","26611    [we, bought, dvd, walking, dinosaurs, nearly, ...\n","37513    [i, watched, film, beginning, end, i, promised...\n","8405     [when, i, i, saw, documentary, a, funny, thing...\n","18798    [i, going, tell, anyone, happens, end, fit, mo...\n","39779    [yes, thing, film, memorable, starred, youngis...\n","Name: review, Length: 40000, dtype: object \n","\n","17332    [hello, i, wondering, anyone, copy, movie, bro...\n","43534    [the, regard, flight, written, performed, bill...\n","27265    [you, realize, watching, exact, same, show, ei...\n","27260    [william, hurt, scuba, diving, scientist, us, ...\n","26133    [surprisingly, enough, movie, redeeming, quali...\n","                               ...                        \n","25341    [ham, handed, homage, honest, hacking, felt, g...\n","49854    [viewed, great, classic, film, greta, garbo, t...\n","48326    [walter, matthau, wonderful, philandering, den...\n","43540    [this, truly, documentary, love, fascinating, ...\n","8044     [i, liked, movie, longer, the, actors, great, ...\n","Name: review, Length: 10000, dtype: object \n","\n","Test Set\n","5636     1\n","25192    1\n","4748     0\n","7084     0\n","1831     1\n","        ..\n","26611    1\n","37513    0\n","8405     0\n","18798    1\n","39779    0\n","Name: sentiment, Length: 40000, dtype: int64 \n","\n","17332    1\n","43534    1\n","27265    0\n","27260    0\n","26133    0\n","        ..\n","25341    0\n","49854    1\n","48326    1\n","43540    1\n","8044     1\n","Name: sentiment, Length: 10000, dtype: int64\n"]}],"source":["x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n","\n","print('Train Set')\n","print(x_train, '\\n')\n","print(x_test, '\\n')\n","print('Test Set')\n","print(y_train, '\\n')\n","print(y_test)"]},{"cell_type":"code","execution_count":null,"id":"4209fdc2-8036-4b8d-a807-8ee3ff600d42","metadata":{"tags":[],"id":"4209fdc2-8036-4b8d-a807-8ee3ff600d42"},"outputs":[],"source":["def get_max_length():\n","    review_length = []\n","    for review in x_train:\n","        review_length.append(len(review))\n","\n","    return int(np.ceil(np.mean(review_length)))"]},{"cell_type":"code","execution_count":null,"id":"bcadebc3-d503-43e9-b7cd-6e010788a143","metadata":{"tags":[],"id":"bcadebc3-d503-43e9-b7cd-6e010788a143","outputId":"150cd737-31de-4d53-8cc4-1d514e787245"},"outputs":[{"name":"stdout","output_type":"stream","text":["130\n","Encoded X Train\n"," [[   1  195  119 ...    0    0    0]\n"," [   1   40   19 ...  100 1247  587]\n"," [ 261  327 7695 ...    0    0    0]\n"," ...\n"," [ 171    1    1 ...   62 1677  735]\n"," [   1   80  284 ...    0    0    0]\n"," [ 320   66    4 ... 2525    6 3191]] \n","\n","Encoded X Test\n"," [[ 4798     1  1529 ...     0     0     0]\n"," [    2  2842  2488 ...     0     0     0]\n"," [   97   863    65 ... 14079   689  3371]\n"," ...\n"," [ 2342  2957   305 ...     0     0     0]\n"," [    8   270   530 ...     0     0     0]\n"," [    1   337     3 ...     0     0     0]] \n","\n","Maximum review length:  130\n"]}],"source":["token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n","token.fit_on_texts(x_train)\n","x_train = token.texts_to_sequences(x_train)\n","x_test = token.texts_to_sequences(x_test)\n","\n","max_length = get_max_length()\n","print(max_length)\n","x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n","x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n","\n","total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n","\n","print('Encoded X Train\\n', x_train, '\\n')\n","print('Encoded X Test\\n', x_test, '\\n')\n","print('Maximum review length: ', max_length)"]},{"cell_type":"code","execution_count":null,"id":"ea847e47-e27e-445e-bc9f-9850b3bea52d","metadata":{"id":"ea847e47-e27e-445e-bc9f-9850b3bea52d","outputId":"69410592-2913-42d1-fbb4-90c1cf454e23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 130, 32)           2947872   \n","                                                                 \n"," conv1d (Conv1D)             (None, 130, 64)           6208      \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 64)                0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 256)               16640     \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 2970977 (11.33 MB)\n","Trainable params: 2970977 (11.33 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n"]}],"source":["EMBED_DIM = 32\n","\n","model = Sequential()\n","model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n","model.add(Conv1D(filters = 64, kernel_size = 3, strides= 1, padding='same', activation= 'relu'))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(units = 256, activation= 'relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"id":"931fb299-0a7f-4f60-920d-bb469a9e2e1c","metadata":{"id":"931fb299-0a7f-4f60-920d-bb469a9e2e1c","outputId":"1a94b0c2-632a-4620-d840-a7100800b9bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","313/313 [==============================] - 29s 87ms/step - loss: 0.4479 - accuracy: 0.7742 - val_loss: 0.3106 - val_accuracy: 0.8671\n","Epoch 2/5\n","313/313 [==============================] - 25s 79ms/step - loss: 0.2056 - accuracy: 0.9204 - val_loss: 0.2932 - val_accuracy: 0.8763\n","Epoch 3/5\n","313/313 [==============================] - 25s 81ms/step - loss: 0.0664 - accuracy: 0.9810 - val_loss: 0.3686 - val_accuracy: 0.8679\n","Epoch 4/5\n","313/313 [==============================] - 25s 80ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.4087 - val_accuracy: 0.8754\n","Epoch 5/5\n","313/313 [==============================] - 25s 81ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 0.4464 - val_accuracy: 0.8750\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x2a54aff3b50>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x_train, y_train,validation_data=(x_test, y_test), batch_size = 128, epochs = 5)"]},{"cell_type":"code","execution_count":null,"id":"02c3244f-00ba-4ac7-8815-702c55ce92bd","metadata":{"id":"02c3244f-00ba-4ac7-8815-702c55ce92bd","outputId":"675a9381-dd4b-4045-9301-bd1ffd80fb76"},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 1s 4ms/step\n","Correct Prediction: 8750\n","Wrong Prediction: 1250\n","Accuracy: 87.5\n"]}],"source":["y_pred =  (model.predict(x_test) > 0.7).astype(\"int32\")\n","\n","true = 0\n","for i, y in enumerate(y_test):\n","    if y == y_pred[i]:\n","        true += 1\n","\n","print('Correct Prediction: {}'.format(true))\n","print('Wrong Prediction: {}'.format(len(y_pred) - true))\n","print('Accuracy: {}'.format(true/len(y_pred)*100))"]},{"cell_type":"code","execution_count":null,"id":"6106e897-92fd-483a-9b53-a027531f1ba0","metadata":{"id":"6106e897-92fd-483a-9b53-a027531f1ba0"},"outputs":[],"source":["model.save('CNN_IMDB_T1.keras')\n","\n"]},{"cell_type":"code","execution_count":null,"id":"dbcf024b-c3c3-4e25-bea7-d9dd1504fdf0","metadata":{"tags":[],"id":"dbcf024b-c3c3-4e25-bea7-d9dd1504fdf0"},"outputs":[],"source":["loaded_model = load_model('CNN_IMDB_T1.keras')"]},{"cell_type":"code","execution_count":null,"id":"ec4e0ce8-46c8-4fb6-a3e5-1f1c8b2fd78c","metadata":{"tags":[],"id":"ec4e0ce8-46c8-4fb6-a3e5-1f1c8b2fd78c","outputId":"a93aba98-547d-4c46-a495-31b826a1d3ee"},"outputs":[{"name":"stdin","output_type":"stream","text":["Movie Review:  It was good\n"]}],"source":["review = str(input('Movie Review: '))"]},{"cell_type":"code","execution_count":null,"id":"9ceaee81-804e-46ef-8059-edca3572dae7","metadata":{"tags":[],"id":"9ceaee81-804e-46ef-8059-edca3572dae7","outputId":"e6ae44b9-ac12-4b84-dde0-7c829c692681"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cleaned:  It was good\n","Filtered:  ['it good']\n"]}],"source":["# Pre-process input\n","regex = re.compile(r'[^a-zA-Z\\s]')\n","review = regex.sub('', review)\n","print('Cleaned: ', review)\n","\n","words = review.split(' ')\n","filtered = [w for w in words if w not in english_stops]\n","filtered = ' '.join(filtered)\n","filtered = [filtered.lower()]\n","\n","print('Filtered: ', filtered)"]},{"cell_type":"code","execution_count":null,"id":"85529685-af2c-410e-b652-24974c0a7f87","metadata":{"tags":[],"id":"85529685-af2c-410e-b652-24974c0a7f87","outputId":"6fed33da-2f9c-4fb3-ee8c-24fa96d0f58b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"]}],"source":["   # no need lower, because already lowered the data in load_data()\n","tokenize_words = token.texts_to_sequences(filtered)\n","tokenize_words = pad_sequences(tokenize_words, maxlen=max_length , padding='post', truncating='post')\n","print(tokenize_words)"]},{"cell_type":"code","execution_count":null,"id":"0a01869a-05c5-4aa1-925b-ee3bd8d8c7a6","metadata":{"tags":[],"id":"0a01869a-05c5-4aa1-925b-ee3bd8d8c7a6","outputId":"dfc6eaf6-5cb5-4e5b-ccf3-f27f77e27a86"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 69ms/step\n","[[0.06568262]]\n"]}],"source":["result = loaded_model.predict(tokenize_words)\n","print(result)"]},{"cell_type":"code","execution_count":null,"id":"02d52138-473c-4285-850a-409691e6bb2f","metadata":{"tags":[],"id":"02d52138-473c-4285-850a-409691e6bb2f","outputId":"8f11b6c3-22cc-4d03-8448-df33bd143bd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["negative\n"]}],"source":["if result >= 0.7:\n","    print('positive')\n","else:\n","    print('negative')"]},{"cell_type":"code","execution_count":null,"id":"ee030191-79d4-4888-956a-05936b790a99","metadata":{"id":"ee030191-79d4-4888-956a-05936b790a99"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}